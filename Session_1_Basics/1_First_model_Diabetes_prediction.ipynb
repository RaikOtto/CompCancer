{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the python installation\n",
    "import sys; print('Python %s on %s' % (sys.version, sys.platform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the Keras installation\n",
    "import os\n",
    "# set the environment variable\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "# make sure we were successful\n",
    "print(os.environ.get(\"KERAS_BACKEND\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Keras backend\n",
    "Keras comes with two different backends:\n",
    "* Tensorflow\n",
    "* CNTK\n",
    "\n",
    "In short, Keras allows you to write high-level code that can then be executed by the framework of your choice (or that of your admin's choice...)\n",
    "For changing the Keras backend, we must access the KERAS_BACKEND environment variable. Allowed values obviously are:\n",
    "* tensorflow\n",
    "* cntk\n",
    "                                                                                                               \n",
    "Note that the third backend Theano is depricated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize tensorflow\n",
    "This is tensorflow specific (and also only needed when working on actual GPUs) but useful to know. \n",
    "By default, tensorflow reserves _all_ memory on _all_ graphic cards it finds on a machine.\n",
    "If you are not the only user (and this will often be the case) this behavior can be painful for all other users.\n",
    "Setting the allow_growth option to true will prevent this from happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit GPU Usage\n",
    "import tensorflow as tf\n",
    "config = tf.compat.v1.ConfigProto() # Tensorflow 2.0 version\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the relevant libraries\n",
    "These are\n",
    "* the keras.models submodule (acts as a container for keras networks)\n",
    "* the keras.layers submodule (here the different types of network layers are defines)\n",
    "* numpy for numerical operations\n",
    "* the confusion_matrix function from sklearn.metrics for evaluation (more on that later)\n",
    "* urllib for loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix a seed\n",
    "For reproducibility, it is a good idea to give the random number generator a fixed seed.\n",
    "\n",
    "Why? As you will see below, we initialize the weights in our layers randomly by using a uniform distribution.\n",
    "By fixing a seed, the \"random\" process will always be the same as long as we execute the code in the same order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "For our first example, we will use the Pima Native Americans Diabetes data set (more on this at the [UCI Machine Learning repository](https://archive.ics.uci.edu/ml/datasets/pima+indians+diabetes)).\n",
    "\n",
    "As is usual in ML applications, the data set comes as a large $N\\times D$ matrix where $N$ denotes the number of samples we have and $D$ is the number of dimensions per sample.\n",
    "Note that in this case, the number of features for a single data point is only $D-1$ since the last column is the class variable that we are trying to predict.\n",
    "\n",
    "For the current data set, the columns are as follows:\n",
    "1. Number of times pregnant \n",
    "2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test \n",
    "3. Diastolic blood pressure (mm Hg) \n",
    "4. Triceps skin fold thickness (mm) \n",
    "5. 2-Hour serum insulin (mu U/ml) \n",
    "6. Body mass index (weight in kg/(height in m)^2) \n",
    "7. Diabetes pedigree function \n",
    "8. Age (years) \n",
    "9. Class variable (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "dataset = numpy.loadtxt('pima-indians-diabetes.csv', delimiter=\",\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create input and output data\n",
    "As said, the input features are the first $D-1$ columns of our data set and the response (or class or label, you name it) is the $D^{\\text{th}}$ column. In our example, we have $D=9$, so column $1$ to $8$ should go int the training data $X$ and the last into the label data $Y$.\n",
    "\n",
    "Note a python subtlety: the range ```0:8``` will exclude the right bound, i.e. will go from $0$ to $7$ (but will be comprised of eight numbers).\n",
    "The ninth column is the class variable and will go into output $Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create first network with Keras\n",
    "We are now done with preparing the data and it is time to construct our first network structure.\n",
    "\n",
    "To begin with, we are building a simple linearly stacked sequence of layers into a model structure.\n",
    "For this we can use the ```Sequential()``` container. More complicated models (e.g. with multiple input sources or multiple output values) can be programatically constructed by using Keras' functional Model class API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we know about the data so far?\n",
    "\n",
    "* we know that we have 8 input feature dimensions (matching the values in columns 1-8 described above)\n",
    "* we know that we have a single output value which is either $0$ or $1$\n",
    "\n",
    "The first fact is encoded in the first call to the ```model.add``` function, which adds new layers to the sequential model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(12, input_dim=8, kernel_initializer='random_normal', activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer structure\n",
    "The above code tells us that we have $8$ input features and so our input layer to the network will have $8$ nodes.\n",
    "The first hidden layer consists of $12$ nodes and is densely connected to the input (signified by the ```Dense``` layer class we are using to create the layer) meaning that every input layer node is connected to every node in the hidden layer.\n",
    "\n",
    "### Intermediate hidden layer\n",
    "The number of nodes in this layer is fairly random. In general, recall that we are trying to learn distributional representations of the data, i.e. as we traverse the layers, we want to find more abstract representations of the data. For this, it can be useful to blow up the space (in our case from $8$ to $12$ dimensions) in which the data can be represented. We then narrow the space again, to group discriminatory attributes from the higher-dimensional space to finally do the regression or classification task at hand.\n",
    "\n",
    "Again, the layer is going to be densely connected to the underlying hidden layer and is initialized using the ```random_uniform``` function. Also, it uses the ```relu``` activation again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(8, kernel_initializer='uniform', activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crafting the output layer\n",
    "Remember that we know from inspecting the data set that we are dealing with a classification task and that there is exactly one class variable that we are trying to predict.\n",
    "\n",
    "This knowledge dictates the structure of the output layer. We will have a single node in this layer. Again, we are using a dense layer, i.e. we connect each node from the underlying hidden layer to the output node. And we will use a different activation function, the ```sigmoid``` (or logistic) that will produce a value in $[0,1]$ that can be interpreted as the predicted probability that the class variable will be one.\n",
    "\n",
    "In the actual prediction task we can use this number to decide for class assignment to class $1$ if the output is larger than $0.5$ or to class $0$ if the value is below that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the model\n",
    "After we have added layers to our sequential model, we need to compile the model by calling the ```compile``` function and providing a loss function and an optimizer.\n",
    "\n",
    "### The loss function\n",
    "The loss function is the objective that gets optimized by the optimizer. In general, the loss function is a function $$\\ell: \\mathcal Y \\times \\mathcal Y \\to \\mathbb R,$$\n",
    "taking the true output value and the predicted value (both of which are elements of the output space $\\mathcal Y$) and computing a _loss_. Obviously, the loss becomes $0$ as soon as the predicted value is equal to the true value.\n",
    "\n",
    "### The optimizer\n",
    "The goal of learning in DNNs (and ANNs as well) is to readjust the weights so that the loss gets smaller (i.e. is minimized). We do this by computing the gradients of the loss function w.r.t. the weights in the networks.\n",
    "While we would need the whole data set to compute one gradient step (the loss function depends on all data points in the data set), we usually use optimizers based on stochastic gradient descent in modern DNN applications.\n",
    "\n",
    "Adam, again as one of the most popular algorithms, combines several advantages of other approaches.\n",
    "\n",
    "Optimizers can be found [here](https://keras.io/optimizers/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "Additional to the required arguments of loss and optimizer, we also provide a ```metrics``` argument that tells the model that we want to keep track of the ```accuracy``` of the network during training and testing. We will see how that works below.\n",
    "\n",
    "## Training the network\n",
    "With a simple call to the ```model.fit``` function, we train the DNN. For this, we provide the training inputs ```X```, the desired outputs ```Y```, the number of ```epochs```, the ```batch_size``` and a level of verbosity.\n",
    "While the input and output should be clear, ```epochs``` tells the model how many times we should traverse the training data set during training and ```batch_size``` tells the optimizer, how many data points it should include in one _mini batch_. The ```verbose``` argument simply tells the framework how much information you want to see during training.\n",
    "\n",
    "**Warning: this may take a moment!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "history = model.fit(\n",
    "    X,\n",
    "    Y,\n",
    "    epochs=1000,\n",
    "    batch_size=10,\n",
    "    verbose=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the outcome\n",
    "After training, we of course want to know, how well we are doing on the data set.\n",
    "For this, we may call the ```model.evaluate``` method which takes in again a number of inputs and outputs and reports the ```loss``` and ```accuracy``` of the network after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the network\n",
    "loss, accuracy = model.evaluate(X, Y)\n",
    "print(\"\\nLoss: %.2f, Accuracy: %.2f%%\" % (loss, accuracy*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "Notice that we have stored the fit in a ```history``` variable. This gives us a nice way of visualizing the behavior of the model during training w.r.t. the loss and the recorded metric, both of which can be accessed via ```history.history['loss']``` and ```history.history['accuracy']``` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'b.', label='Training loss')\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# accuracy\n",
    "\n",
    "plt.clf()   # clear figure\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "\n",
    "plt.plot(epochs, acc, 'b.', label='Training acc')\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate predictions\n",
    "probabilities = model.predict(X)\n",
    "predictions = [float(x>0.5) for x in probabilities]\n",
    "\n",
    "cm = confusion_matrix(Y, predictions)\n",
    "\n",
    "tp = float(cm[1,1])\n",
    "fp = float(cm[0,1])\n",
    "tn = float(cm[0,0])\n",
    "fn = float(cm[1,0])\n",
    "print (\"True positives:  %.0f\" % tp)\n",
    "print (\"False positives: %.0f\" % fp)\n",
    "print (\"True negatives:  %.0f\" % tn)\n",
    "print (\"False negatives: %.0f\" % fn)\n",
    "\n",
    "prec = tp/(tp+fp)\n",
    "rec = tp/(tp+fn)\n",
    "f1 = (2*prec*rec)/(prec+rec)\n",
    "print (\"Precision: %.3f\" % prec)\n",
    "print (\"Recall: %.3f\" % rec)\n",
    "print (\"F1: %.3f\" % f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations\n",
    "\n",
    "You have just trained your (perhaps) first DL model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p36)",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
