{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries, load data, preprocess data\n",
    "\n",
    "Start by importing numpy and setting a seed for the computer's pseudorandom number generator. This allows us to reproduce the results from our script, first some sanity tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys; print('Python %s on %s' % (sys.version, sys.platform)) # ensure the correct python version\n",
    "import os\n",
    "\n",
    "# Ensure Keras is available\n",
    "print(os.environ.get(\"KERAS_BACKEND\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import libraries and modules\n",
    "import numpy as np\n",
    "np.random.seed(123)  # for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the sequential model type from Keras. This is simply a linear stack of neural network layers, frequently utilized for Feed Foward Neural Network (FFNNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, Activation, Flatten # \"Core\" layers of Keras, frequently utilized\n",
    "from keras.layers import Convolution2D, MaxPooling2D # load the convolutional layer for image analysis\n",
    "from keras.utils import np_utils # for data transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have everything we need to build our neural network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's import the \"core\" layers from Keras. These are the layers that are used in almost any neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load pre-shuffled MNIST data into train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll import the CNN layers from Keras. These are the convolutional layers that will help us efficiently train on image data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Preprocess input data\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll import some utilities. This will help us transform our data later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Preprocess class labels\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have everything we need to build our neural network architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the DL model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Keras library conveniently MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    " \n",
    "# Load pre-shuffled MNIST data into train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the shape of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print( X_train.shape)\n",
    "# (60000, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, so it appears that we have 60,000 samples in our training set, and the images are 28 pixels x 28 pixels each. We can confirm this by plotting the first sample in matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(X_train[0,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape input dataPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28) #6000, 1, 28, 28\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert data type and normalize values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's take a look at the shape of our class label data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( y_train.shape)\n",
    "print(y_train[1])\n",
    "for i in range(10):\n",
    "    print (y_train[i])\n",
    "# (60000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problematic, we should have 10 different classes, one for each digit, but it looks like we only have a 1-dimensional array. Let's take a look at the labels for the first 10 training samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)\n",
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to define our model architecture.\n",
    "Let's start by declaring a sequential model format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Define model architecture\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Convolution2D(32, (3, 3), activation='relu', input_shape=(1,28,28), data_format='channels_first'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input shape parameter should be in the shape of 1 sample each. In this case, it's the same (1, 28, 28) that corresponds to  the (depth, width, height) of each digit image.\n",
    "\n",
    "The first 3 parameters represent the number of convolution filters to use, the number of rows in each convolution kernel, and the number of columns in each convolution kernel, respectively.\n",
    "\n",
    "*Note: The step size is (1,1) by default, and it can be tuned using the 'subsample' parameter.\n",
    "\n",
    "We can confirm this by printing the shape of the current model output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Here, add another Convolution2D layer, just like before, but this time without input_shape and data_format to obtain the correct dimensions\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to highlight the Dropout layer. This is a method for regularizing our model in order to prevent overfitting.\n",
    "\n",
    "MaxPooling2D is a way to reduce the number of parameters in our model by sliding a 2x2 pooling filter across the previous layer and taking the max of the 4 values in the 2x2 filter.\n",
    "\n",
    "Add a fully connected layer and then the output layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "#Your code here \n",
    "# Here, add a Dense layer of size 128 and relu activation funciton\n",
    "# Here, add another dropout layer with dropout value of your choice\n",
    "# Here, add dense layer in output, remember that you want to predict 10 classes. Use a softmax function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Dense layers, the first parameter is the output size of the layer. Keras automatically handles the connections between layers.\n",
    "\n",
    "Note that the final layer has an output size of 10, corresponding to the 10 classes of digits.\n",
    "\n",
    "Also note that the weights from the Convolution layers must be flattened (made 1-dimensional) before passing them to the fully connected Dense layer.\n",
    "\n",
    "Now, we can compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Compile model\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 7. Fit model on training data\n",
    "batch_size =20 # your batch size here\n",
    "number_epochs = 20# your batch size here\n",
    "\n",
    "\n",
    "# your code here\n",
    "history = model.fit(\n",
    "    # input data\n",
    "    X_train,\n",
    "    # input labels\n",
    "    Y_train,\n",
    "    # your batch size here\n",
    "    batch_size,\n",
    "    # number epochs\n",
    "    number_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 8 Evaluate the network\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "loss=score[0]\n",
    "accuracy=score[1]\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "# print your loss and accuracy\n",
    "print(\"\\nLoss: %.2f, Accuracy: %.2f%%\" % (loss, accuracy*100))\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (decrease) of loss function value over epochs, like in the first tutorial\n",
    "# loss function\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'b.', label='Training loss')\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "\n",
    "plt.clf()   # clear figure\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "\n",
    "plt.plot(epochs, acc, 'b.', label='Training acc')\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate predictions\n",
    "from numpy.random import randint\n",
    "from numpy import argmax\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "probabilities = model.predict(X_train)\n",
    "print(len(probabilities))# 6000\n",
    "print(probabilities[0]) #[1.6255919e-16 1.2972479e-14 3.1088004e-16 7.7943166e-04 3.0686767e-19 9.9922037e-01 1.2762679e-12 3.3330028e-17 3.6068312e-11 1.9672434e-07]\n",
    "probabilities_R=argmax(probabilities, axis=1)\n",
    "print(len(probabilities_R)) # 6000\n",
    "print(probabilities_R[0]) #5\n",
    "Y_R=argmax(Y_train, axis=1)\n",
    "cm = confusion_matrix(Y_R, probabilities_R)\n",
    "\n",
    "tp = float(cm[1,1])\n",
    "fp = float(cm[0,1])\n",
    "tn = float(cm[0,0])\n",
    "fn = float(cm[1,0])\n",
    "print (\"True positives:  %.0f\" % tp)\n",
    "print (\"False positives: %.0f\" % fp)\n",
    "print (\"True negatives:  %.0f\" % tn)\n",
    "print (\"False negatives: %.0f\" % fn)\n",
    "\n",
    "prec = tp/(tp+fp)\n",
    "rec = tp/(tp+fn)\n",
    "f1 = (2*prec*rec)/(prec+rec)\n",
    "print (\"Precision: %.3f\" % prec)\n",
    "print (\"Recall: %.3f\" % rec)\n",
    "print (\"F1: %.3f\" % f1)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "print(X_train.shape) #(60000, 1, 28, 28)\n",
    "import copy\n",
    "X_train1=copy.deepcopy(X_train)\n",
    "X_train1 = X_train1.reshape(X_train.shape[0], 28, 28)\n",
    "print(X_train1.shape)  #(60000,  28, 28)\n",
    "plt.imshow(X_train1[0,:,:])\n",
    "example=X_train[0,:,:,:]  \n",
    "example=example.reshape(1,1,28,28)\n",
    "#print(example)\n",
    "print(example.shape)#(1,1, 28, 28)\n",
    "example_p = model.predict(example)\n",
    "print(example_p)\n",
    "print(argmax(example_p, axis=1)) ##[5]\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import randint\n",
    "from numpy import argmax\n",
    "from keras.utils.np_utils import to_categorical\n",
    "k = 8\n",
    "n = 20\n",
    "x = randint(0, k, (n,))\n",
    "print(x)\n",
    "#Y_train = np_utils.to_categorical(y_train, 10)\n",
    "k1=to_categorical(x,k)\n",
    "print(k1)\n",
    "print(argmax(k1, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model.save('2_Image_MNIST_Classification.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "print (model)\n",
    "#del model  # deletes the existing model\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "model1 = load_model('2_Image_MNIST_Classification.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from numpy import array\n",
    "import numpy as np\n",
    "img = Image.open(\"5.jpg\")\n",
    "arr = array(img)\n",
    "#Converting a numpy array to a picture is like this:\n",
    "print(img.size)\n",
    "print(arr.shape)#(209, 211, 3)\n",
    "img = Image.fromarray(arr)\n",
    "img.save(\"output.png\")\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "img_greyscale = Image.open('5.jpg').convert('LA')\n",
    "#img_greyscale.save('greyscale.png')\n",
    "new_img_greyscale = img_greyscale.resize((28, 28))\n",
    "plt.imshow(new_img_greyscale, cmap=plt.get_cmap('gray'), vmin=0, vmax=1)\n",
    "plt.show()\n",
    "arr = array(new_img_greyscale)\n",
    "print(new_img_greyscale.size) #(28, 28)\n",
    "print(arr.shape)#(28, 28, 2)\n",
    "\n",
    "#new_image = img.resize((28, 28))\n",
    "#print(new_image.size)\n",
    "#arr1=array(new_image)\n",
    "#print(arr1.shape)\n",
    "#real_ex=np.reshape(array(new_image),(1,28,28))\n",
    "#print(real_ex.shape)#(1,1, 28, 28)\n",
    "#real_ex.shape=real_ex.reshape(1,1, 28, 28)\n",
    "from keras.models import load_model\n",
    "model1 = load_model('2_Image_MNIST_Classification.h5')\n",
    "real_ex_p = model1.predict(real_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p36)",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
