{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"banner-small.png\">\n",
    "</center>\n",
    "\n",
    "<h1>Tutorial: Deep Neural Networks and Explanations in PyTorch</h1>\n",
    "\n",
    "<p>The goal of this tutorial is to train a neural network to predict an image dataset with few labels. For this, we consider a subset of the \"Labeled Faces in the Wild\" dataset, readily available in Scikit-Learn:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.datasets\n",
    "\n",
    "D = sklearn.datasets.fetch_lfw_people(\n",
    "    slice_=(slice(68, 197, None), slice(68, 197, None)),\n",
    "    resize=0.5, min_faces_per_person=40,color=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc = len(D.target_names)\n",
    "\n",
    "print('Number of examples: %d'%len(D['images']))\n",
    "print('Number of classes: %d'%nc)\n",
    "\n",
    "import torch\n",
    "\n",
    "Itrain = D.data[::2].reshape(-1,64,64,3)/255.0\n",
    "Itest = D.data[1::2].reshape(-1,64,64,3)/255.0\n",
    "Ttrain = torch.LongTensor(D.target[::2])\n",
    "Ttest = torch.LongTensor(D.target[1::2])\n",
    "Xtrain = torch.FloatTensor(Itrain.transpose(0,3,1,2)*3-1.5)\n",
    "Xtest = torch.FloatTensor(Itest.transpose(0,3,1,2)*3-1.5)\n",
    "print(Itrain.shape)\n",
    "print(Itest.shape)\n",
    "print(Ttrain.shape)\n",
    "print(Ttest.shape)\n",
    "print(Xtrain.shape)\n",
    "print(Xtest.shape)\n",
    "#print(Ytrain.shape)\n",
    "#print(Ytest.shape)\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def images(start):\n",
    "    f = plt.figure(figsize=(16,2))\n",
    "    for j in range(8):\n",
    "        p = f.add_subplot(1,8,j+1)\n",
    "        p.imshow(Itest[start:start+8][j])\n",
    "        p.set_xlabel(D.target_names[Ttest[start:start+8][j]].split(\" \")[-1])\n",
    "        p.set_xticks([])\n",
    "        p.set_yticks([])\n",
    "\n",
    "images(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part 1: Training a convolutional neural network</h2>\n",
    "\n",
    "<p>We now consider a simple convolutional neural network composed of 4 convolution layers, rectified linear units, and three pooling stages.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "net1 = nn.Sequential(\n",
    "    nn.Conv2d(  3, 10, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2),\n",
    "    nn.Conv2d( 10, 25, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2),\n",
    "    nn.Conv2d( 25,100, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2),\n",
    "    nn.Conv2d(100, nc, 8)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function takes a neural network and prints the training and test accuracy on the face classification data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printacc(net):\n",
    "    net.eval()\n",
    "    Ytrain = net.forward(Xtrain).view(-1,nc).data\n",
    "    Ytest = net(Xtest).view(-1,nc).data\n",
    "    acctrain = torch.mean((torch.max(Ytrain,dim=1)[1] == Ttrain).type(torch.FloatTensor)).item()\n",
    "    acctest  = torch.mean((torch.max(Ytest, dim=1)[1] == Ttest).type(torch.FloatTensor)).item()\n",
    "    print('train: %.3f  test: %.3f' %(acctrain,acctest))\n",
    "    net.train()\n",
    "    \n",
    "printacc(net1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network initially predicts at random, hence a low training and test accuracy. The following function trains the neural network on the training data using stochastic gradient descent for a certain number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "def train(net,nbit=2500):\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.005, momentum=0.9)\n",
    "    \n",
    "    for i in range(nbit+1):\n",
    "\n",
    "        R = numpy.random.permutation(len(Xtrain))[:25]\n",
    "\n",
    "        xr, tr = Xtrain[R]*1, Ttrain[R]*1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        criterion(net.forward(xr).view(-1,nc),tr).backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % (nbit//5) == 0: printacc(net)\n",
    "\n",
    "train(net1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout training, the prediction accuracy becomes maximum on the training set but reaches some saturation on the test set. This suggests that the neural network is sufficiently large and that the main bottleneck is statistical overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part 2: Improving generalization</h2>\n",
    "\n",
    "We would like to improve the generalization ability of the neural network.\n",
    "\n",
    "<h3>Dropout</h3>\n",
    "\n",
    "As a first try, we add a dropout layer in the network. Note that dropout layers are the most effective in the last layers of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2 = nn.Sequential(\n",
    "    nn.Conv2d(  3, 10, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2),\n",
    "    nn.Conv2d( 10, 25, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2),\n",
    "    nn.Conv2d( 25,100, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2),\n",
    "    nn.Dropout2d(),\n",
    "    nn.Conv2d(100, nc, 8)\n",
    ")\n",
    "\n",
    "train(net2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training accuracy still reaches 1.0, but thanks to dropout, the test accuracy is now sensibly higher.\n",
    "\n",
    "<h3>Transfer learning</h3>\n",
    "\n",
    "We now investigate a second technique to improve the test accuracy, which is to use a neural network that has been pretrained on some generic computer vision task with many labels, e.g. ImageNet. We load the 17 first layers of the VGG-16 network pretrained on ImageNet and apply these layers to our face data to generate features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "vgg16 = torchvision.models.vgg16(pretrained=True).features\n",
    "features = nn.Sequential(*list(vgg16)[:17])\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretrained neural networks are usually preforming highly nonlinear and high-dimensional mappings. Hence, a linear model trained on top of these features may be sufficient. We train here a simple logistic regressor with scikit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.linear_model\n",
    "\n",
    "lr = sklearn.linear_model.LogisticRegression()\n",
    "Ftrain = features.forward(Xtrain).data\n",
    "lr.fit(numpy.array(Ftrain.reshape(len(Xtrain),-1)),Ttrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A logistic regressor can be seen as a one-layer neural network trained with cross entropy. Hence, we can convert our logistic regression model into a neural network layer and append it to our sequence of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.FloatTensor(lr.__dict__['coef_'].reshape(nc,256,8,8))\n",
    "bias = torch.FloatTensor(lr.__dict__['intercept_'])\n",
    "topconv = nn.Conv2d(256,nc,8)\n",
    "topconv.weight = nn.Parameter(weight)\n",
    "topconv.bias = nn.Parameter(bias)\n",
    "net3 = nn.Sequential(*(list(features)+[topconv]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network can now be tested for its accuracy on the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printacc(net3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training accuracy is still maximum, but this time the test accuracy has dramatically increased. This suggests that the generic VGG-16 visual features are very useful for our classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Part 3: Explaining predictions with LRP</h3>\n",
    "\n",
    "<p> We now would like to get insight into the predictions of our models by applying the LRP method. The LRP-0, LRP-$\\epsilon$, and LRP-$\\gamma$ rules described in the <a href=\"https://doi.org/10.1007/978-3-030-28954-6_10\">LRP overview paper</a> (Section 10.2.1) for propagating relevance on the lower layers are special cases of the more general propagation rule</p>\n",
    "\n",
    "<p>\n",
    "$$\n",
    "R_j = \\sum_k \\frac{a_j \\rho(w_{jk})}{\\epsilon + \\sum_{0,j} a_j \\rho(w_{jk})} R_k\n",
    "$$\n",
    "</p>\n",
    "\n",
    "<p>(cf. Section 10.2.2), where $\\rho$ is a function that transform the weights, and $\\epsilon$ is a small positive increment. We now come to the practical implementation of this general rule. It can be decomposed as a sequence of four computations:</p>\n",
    "\n",
    "<p>\n",
    "\\begin{align*}\n",
    "\\forall_k:&~z_k = {\\textstyle \\epsilon + \\sum_{0,j}} a_j \\rho(w_{jk}) & (\\text{step }1)\\\\\n",
    "\\forall_k:&~s_k = R_k / z_k \\qquad & (\\text{step }2)\\\\\n",
    "\\forall_j:&~c_j = {\\textstyle \\sum_k} \\rho(w_{jk}) s_k \\qquad & (\\text{step }3)\\\\\n",
    "\\forall_j:&~R_j = a_j \\cdot c_j \\qquad & (\\text{step }4)\n",
    "\\end{align*}\n",
    "</p>\n",
    "\n",
    "<p>The layer-wise relevance propagation procedure then consists of iterating over the layers in reverse order, starting from the top layer towards the first layers, and at each layer, applying this sequence of computations.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def newlayer(layer,g):\n",
    "\n",
    "    layer = copy.deepcopy(layer)\n",
    "\n",
    "    try: layer.weight = nn.Parameter(g(layer.weight))\n",
    "    except AttributeError: pass\n",
    "\n",
    "    try: layer.bias   = nn.Parameter(g(layer.bias))\n",
    "    except AttributeError: pass\n",
    "\n",
    "    return layer\n",
    "\n",
    "def LRP(layers,X,T):\n",
    "    \n",
    "    L = len(layers)\n",
    "    \n",
    "    # -------------------------------------------------------------\n",
    "    # Set up activations and relevance scores\n",
    "    # -------------------------------------------------------------\n",
    "    A = [X]+[None]*L\n",
    "    for l in range(L): A[l+1] = layers[l].forward(A[l])\n",
    "    R = [None]*L + [A[-1].data*T]\n",
    "\n",
    "    for l in range(0,L)[::-1]:\n",
    "        \n",
    "        A[l] = (A[l].data).requires_grad_(True)\n",
    "\n",
    "        \n",
    "        # -------------------------------------------------------------\n",
    "        # Special case: first layer\n",
    "        # -------------------------------------------------------------\n",
    "        if l==0:\n",
    "            \n",
    "            A[0] = (A[0].data).requires_grad_(True)\n",
    "\n",
    "            lb = (A[0].data*0-1.5).requires_grad_(True)\n",
    "            hb = (A[0].data*0+1.5).requires_grad_(True)\n",
    "\n",
    "            z = layers[0].forward(A[0]) + 1e-9                                     # step 1 (a)\n",
    "            z -= newlayer(layers[0],lambda p: p.clamp(min=0)).forward(lb)    # step 1 (b)\n",
    "            z -= newlayer(layers[0],lambda p: p.clamp(max=0)).forward(hb)    # step 1 (c)\n",
    "            s = (R[1]/z).data                                                      # step 2\n",
    "            (z*s).sum().backward(); c,cp,cm = A[0].grad,lb.grad,hb.grad            # step 3\n",
    "            R[0] = (A[0]*c+lb*cp+hb*cm).data                                       # step 4\n",
    "\n",
    "            \n",
    "        # -------------------------------------------------------------\n",
    "        # General convolution and pooling layers\n",
    "        # -------------------------------------------------------------\n",
    "        elif isinstance(layers[l],torch.nn.Conv2d) or isinstance(layers[l],torch.nn.MaxPool2d):\n",
    "\n",
    "            rho = lambda p: p + (0.25 if l < L-1 else 0.0)*p.clamp(min=0); incr = lambda z: z+1e-9\n",
    "\n",
    "            z = incr(newlayer(layers[l],rho).forward(A[l])) # step 1\n",
    "            s = (R[l+1]/z).data                                   # step 2\n",
    "            (z*s).sum().backward(); c = A[l].grad                 # step 3\n",
    "            R[l] = (A[l]*c).data                                  # step 4\n",
    "\n",
    "        # -------------------------------------------------------------\n",
    "        # ReLU layers (pass through)\n",
    "        # -------------------------------------------------------------\n",
    "        else:\n",
    "\n",
    "            R[l] = R[l+1]\n",
    "            \n",
    "    return R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the LRP function to find evidence found by each network to explain class membership."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "my_cmap = plt.cm.seismic(numpy.arange(plt.cm.seismic.N))\n",
    "my_cmap[:,0:3] *= 0.85\n",
    "my_cmap = ListedColormap(my_cmap)\n",
    "\n",
    "def heatmaps(net,start):\n",
    "\n",
    "    R = LRP(list(net),Xtest[start:start+8],torch.eye(nc)[Ttest[start:start+8]].view(-1,nc,1,1))[0]\n",
    "    R = numpy.array(R)\n",
    "    \n",
    "    Rmax = numpy.abs(R).max()\n",
    "    Rsmax = numpy.abs(R).sum(axis=1).max()\n",
    "    \n",
    "    f = plt.figure(figsize=(16,2))\n",
    "    for j in range(8):\n",
    "        p = f.add_subplot(1,8,j+1)\n",
    "        p.set_xticks([])\n",
    "        p.set_yticks([])\n",
    "        p.imshow(R[j].sum(axis=0),cmap=my_cmap,vmin=-Rsmax,vmax=Rsmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images(0)\n",
    "heatmaps(net1,0)\n",
    "heatmaps(net2,0)\n",
    "heatmaps(net3,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last network (based on pretrained VGG-16) is not only the one with highest accuracy, its decisions are also supported by a broader set of features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p27)",
   "language": "python",
   "name": "conda_pytorch_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
