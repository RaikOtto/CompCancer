{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import lifelines\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.lines as mlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoding colorectal cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load the mRNA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrna = pd.read_csv(os.path.join(DATADIR, \"crc-mrna.csv.gz\"), index_col=0)\n",
    "mrna.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the skewed distributions of the features (genes). These will need to be normalized in some way to get them to work with neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "for gene in np.random.choice(mrna.index, 10, replace=False):\n",
    "    sns.distplot(mrna.loc[gene], ax=ax, label=gene)\n",
    "plt.legend()\n",
    "ax.set_xlabel(\"RPKM\")\n",
    "ax.set_xlim(-1,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, we'll log-transform RPKM values to get them to look more normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrna_log = np.log2(mrna+1)\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "for gene in np.random.choice(mrna.index, 10, replace=False):\n",
    "    sns.distplot(mrna_log.loc[gene], ax=ax, label=gene)\n",
    "plt.legend()\n",
    "ax.set_xlabel(\"log(RPKM+1)\")\n",
    "# ax.set_xlim(-1,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll scale these so they all are closer to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrna_scaled = pd.DataFrame(\n",
    "    scale(mrna_log.copy(), axis=1),\n",
    "    index=mrna.index,\n",
    "    columns=mrna.columns\n",
    ")\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "for gene in np.random.choice(mrna.index, 10, replace=False):\n",
    "    sns.distplot(mrna_scaled.loc[gene], ax=ax, label=gene)\n",
    "plt.legend\n",
    "ax.set_xlabel(\"scaled RPKM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, load the subtype data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtypes = pd.read_csv(os.path.join(DATADIR, \"crc-subtypes.csv.gz\"), index_col=0)\n",
    "subtypes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to get a rough idea of what this data looks like, let's make a scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary mapping sample IDs to their subtype, from the table above\n",
    "sample2subtype = dict(zip(subtypes.index, subtypes.cms_label))\n",
    "\n",
    "# Define a dictionary mapping subtypes to colors\n",
    "subtype2color = dict(zip(sorted(subtypes.cms_label.unique()), sns.color_palette()))\n",
    "\n",
    "# Define an array of colors for all samples, using the two dictionaries defined just above\n",
    "sample_colors = [subtype2color.get(sample2subtype.get(s, \"NOLBL\")) for s in mrna.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "# 2D PCA and scatter plot\n",
    "pd.DataFrame(\n",
    "    PCA(2).fit_transform(mrna_log.T),\n",
    "    index=mrna_scaled.columns,\n",
    "    columns=[\"PC1\", \"PC2\"]\n",
    ").plot.scatter(\"PC1\", \"PC2\", color=sample_colors, s=40, ax=ax)\n",
    "\n",
    "# Add figure legend\n",
    "cms_legend_handles = [mlines.Line2D([],[], label=label, color=color, markersize=10, marker=\"o\", linewidth=0) for label, color in subtype2color.items()]\n",
    "ax.legend(handles=cms_legend_handles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For another quick look at the data, let's explore these subtypes' survival rates. First, load the survival data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survival = pd.read_csv(os.path.join(DATADIR, \"crc-survival.csv.gz\"), index_col=0)\n",
    "survival.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "# Instantiate a Kaplam Meier Fitter object\n",
    "kmf = lifelines.KaplanMeierFitter()\n",
    "\n",
    "# For each CMS subtype, plot the KM estimate\n",
    "for subtype in [f\"CMS{i}\" for i in range(1,5)]:\n",
    "    # Select the patients with this subtype\n",
    "    patients = subtypes[subtypes.cms_label==subtype].index\n",
    "    \n",
    "    # Select the patients with this subtype who also have survival data in the table\n",
    "    patients_with_survival = set(patients) & set(survival.index)\n",
    "    s = survival.loc[patients_with_survival]\n",
    "    \n",
    "    # Fit and plot the KM estimate\n",
    "    kmf.fit(s.duration, s.observed, label=subtype)\n",
    "    kmf.plot(ax=ax, ci_show=False, show_censors=True, linewidth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "\n",
    "# Fitting an autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Set some sensible limits when running on CPU on a shared server\n",
    "tf.keras.backend.clear_session()\n",
    "tf.config.threading.set_intra_op_parallelism_threads(2)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(2)\n",
    "\n",
    "# Print the tensorflow version\n",
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = mrna.shape[0] # The dimension of the input layer, is the number of genes\n",
    "embedding_size = 2 # The embedding size, of the reduced dimension (dimension of latent soace)\n",
    "\n",
    "# Definint the autoencoder layers\n",
    "mrna_input = keras.layers.Input(shape=(input_size,), name=\"input\")\n",
    "hidden = keras.layers.Dense(embedding_size, activation=\"sigmoid\", name=\"hidden\")(mrna_input)\n",
    "output = keras.layers.Dense(input_size, activation=\"sigmoid\", name=\"reconstruction\")(hidden)\n",
    "\n",
    "# Defining the end-to-end autoencoder model\n",
    "ae = tf.keras.Model(mrna_input, output, name=\"Vanilla autoencoder\")\n",
    "\n",
    "# Compiling the model, using the Adam optimized, and MSE loss\n",
    "ae.compile(optimizer=tf.optimizers.Adam(), loss=tf.losses.mean_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training the model, let's split the data into training and testing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing data, keeping 90% for training\n",
    "x_train, x_test = train_test_split(mrna_scaled.T, train_size=.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Using Keras to train the autoencoder. Not that x==y, because we are teaching it to reconstruct!\n",
    "ae.fit(x=x_train, y=x_train, validation_data=[x_test, x_test], epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training loss\n",
    "pd.DataFrame(ae.history.history).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now since we're satisfied with the overall performance of this autoencoder, let's split it up in encoder and decoder. After all, it's mainly the encoder we're interested in, in order to perform dimensionality reduction..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The encoder model is a keras model like the full autoencoder\n",
    "# It reuses the same layers, but only goes from input to latent space!\n",
    "encoder = keras.Model(mrna_input, hidden, name=\"Encoder\")\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dataframe holding autoencoded data\n",
    "z = pd.DataFrame(\n",
    "    encoder.predict(mrna_scaled.T), # using the encoder model's predict() method!\n",
    "    index=mrna_scaled.columns,\n",
    "    columns=[f\"LF{i}\" for i in range(1, embedding_size+1)]\n",
    ")\n",
    "\n",
    "# a 2D scatter plot\n",
    "z.plot.scatter(\"LF1\", \"LF2\", color=sample_colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not super impressive. How about using a larger network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = mrna.shape[0]\n",
    "embedding_size = 100\n",
    "\n",
    "# Define a function that returns autoencoders and encoders\n",
    "def make_vanilla_autoencoder(input_size, embedding_size):\n",
    "    # Define the layers\n",
    "    mrna_input = keras.layers.Input(shape=(input_size,), name=\"input\")\n",
    "    hidden = keras.layers.Dense(embedding_size, activation=\"sigmoid\", name=\"hidden\")(mrna_input)\n",
    "    output = keras.layers.Dense(input_size, activation=\"sigmoid\", name=\"reconstruction\")(hidden)\n",
    "    \n",
    "    # Define the end-to-end autoencoder model, and compile it so it can be trained\n",
    "    ae = tf.keras.Model(mrna_input, output, name=\"Vanilla autoencoder\")\n",
    "    ae.compile(optimizer=tf.optimizers.Adam(), loss=tf.losses.mean_squared_error)\n",
    "    \n",
    "    # Define the input-hidden Encoder network\n",
    "    encoder = keras.Model(mrna_input, hidden, name=\"Encoder\")\n",
    "    \n",
    "    # Return both the full autoencoder model, and the encoder network\n",
    "    return ae, encoder\n",
    "\n",
    "ae, encoder = make_vanilla_autoencoder(input_size, embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ae.fit(x=x_train, y=x_train, validation_data=[x_test, x_test], epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(ae.history.history).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, it looks like the validation loss is actually increasing, though the training loss is decreasing, after approximately epoch 400:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(ae.history.history).plot()\n",
    "plt.xlim(0,700)\n",
    "plt.ylim(.575,.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good way to deal with this is by early stopping. In tensorflow 2.0 / keras, we have this easy-to-use callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define new autoencoder and encoder models using the function we wrote above\n",
    "ae, encoder = make_vanilla_autoencoder(input_size, embedding_size)\n",
    "\n",
    "# Fit the model, but this time also use the callbacks argument\n",
    "ae.fit(\n",
    "    x=x_train,\n",
    "    y=x_train,\n",
    "    validation_data=[x_test, x_test],\n",
    "    epochs=1000,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping( # EarlyStopping is conveniently implemented as a standard keras callback\n",
    "            monitor=\"val_loss\", # We want early stopping to look at the validation loss\n",
    "            patience=3 # and wait 3 epochs before truly stopping\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(ae.history.history).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the model stopped training after the validation loss leveled off. Now, let's use it, by using the encoder network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dataframe with the latent factor representation of the input\n",
    "z = pd.DataFrame(\n",
    "    encoder.predict(mrna_scaled.T), # Using the encoder's predict() function!\n",
    "    index=mrna_scaled.columns,\n",
    "    columns=[f\"LF{i}\" for i in range(1, embedding_size+1)]\n",
    ")\n",
    "z.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as `z` has 100 latent factors in it, let's plot a PCA of those, in order to get a 2D visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "# 2D PCA of the latent space and a scatter plot\n",
    "pd.DataFrame(\n",
    "    PCA(2).fit_transform(z),\n",
    "    columns=[\"PC1\", \"PC2\"],\n",
    "    index=z.index\n",
    ").plot.scatter(\n",
    "    \"PC1\",\n",
    "    \"PC2\",\n",
    "    color=sample_colors,\n",
    "    s=40,\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.legend(handles=cms_legend_handles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, using a simple autoencoder, we've reduced the dimensionality from 1,000 genes to 100 latent factors. Then, we used PCA to get 2 dimensions, which we plot here. Whether or not these are better than PCA on the original data is left as an exercise.\n",
    "\n",
    "However, we could also use this reduced dimension latent space to define our own clusters, i.e. CRC subtypes. For instance, using k-means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use k-means (k=4) to define clusters of the data\n",
    "ae_clusters = pd.Series(KMeans(4).fit_predict(z), index=z.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many samples are in each cluster?\n",
    "ae_clusters.value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary mapping each sample to its assigned cluster\n",
    "sample2cluster = dict(zip(ae_clusters.index, ae_clusters))\n",
    "\n",
    "# Defien a dictionary mapping clusters to colors\n",
    "cluster2color = dict(zip(sorted(ae_clusters.unique()), sns.color_palette()))\n",
    "\n",
    "# Define an array with each sample's cluster color, according to the dicts above\n",
    "sample_colors_ae_clusters = [cluster2color.get(sample2cluster.get(s)) for s in ae_clusters.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "# 2D PCA and scatter plot\n",
    "pd.DataFrame(\n",
    "    PCA(2).fit_transform(z),\n",
    "    columns=[\"PC1\", \"PC2\"],\n",
    "    index=z.index\n",
    ").plot.scatter(\n",
    "    \"PC1\",\n",
    "    \"PC2\",\n",
    "    color=sample_colors_ae_clusters, # colored by the cluster colors\n",
    "    s=40,\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "# Adding a figure legend\n",
    "ae_clusters_legend_handles = [mlines.Line2D([],[], label=label, color=color, markersize=10, marker=\"o\", linewidth=0) for label, color in cluster2color.items()]\n",
    "ax.legend(handles=ae_clusters_legend_handles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So using k-means, we've split the patients into four groupings. Do these groupings reveal anything interesting? Let's for instance examine their survival statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "kmf = lifelines.KaplanMeierFitter()\n",
    "\n",
    "# For each cluster\n",
    "for cl in ae_clusters.unique():\n",
    "    # The patienrs in this cluster\n",
    "    patients = ae_clusters[ae_clusters==cl].index\n",
    "    \n",
    "    # The patients in this cluster who also have survival data\n",
    "    patients_with_survival = set(patients) & set(survival.index)\n",
    "    s = survival.loc[patients_with_survival]\n",
    "    \n",
    "    # Estimate and plot kaplan meier curve\n",
    "    kmf.fit(s.duration, s.observed, label=cl)\n",
    "    kmf.plot(ax=ax, ci_show=False, show_censors=True, linewidth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "\n",
    "# Exercises\n",
    "\n",
    "## Regularization\n",
    "\n",
    "### Exercise 1 (EASY): add l1 regularization to the autoencoder\n",
    "\n",
    "A vanilla autoencoder like this is probably going to produce a fairly dense network, i.e. one where most nodes are connected with most nodes. We can examine the weights of the network like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(ae.layers[1].get_weights()[0].flatten(), kde=False)\n",
    "plt.xlabel(r\"$\\|w\\|$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This distribution of weights is approximately normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: `keras.layers.Dense?`, or https://keras.io/regularizers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 (EASY): De-noising autoencoder (DAE)\n",
    "\n",
    "Recall that a denoising autoencoder adds noise to the input **but not to the output** so that it learns to reconstruct a noise-free sample from noisy input. The DAE learns not only to reconstruct noise free samples, but because all noise applied to an input should result in the same output, it also results in the same latent space representation. Hence, the DAE learns that samples which are _within noise_ of each other, should have the same latent representations. This feature makes DAEs learn smooth manifolds, a great advantage over vanilla AEs.\n",
    "\n",
    "A naive way to implement a DAE is simply to add noise to `x` for the input, and train it using `model.fit(x+noise, x)`. This approach, however, misses out on the manifold-building advantages of DAEs, as each sample only gets one characteristic \"noise\" attached to it, no matter how many epochs we train it for.\n",
    "\n",
    "**Implement a DAE which does not have this drawback**.\n",
    "\n",
    "Hint: `keras.layers.GaussianNoise?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 (CHALLENGING): Variational Autoencoder (VAE)\n",
    "\n",
    "Recall that a variational autoencoder learns **distributions** for latent factors, rather than a deterministic mapping, i.e. $z \\sim q(z|x,w)$. Setting it as a normally distributed variable, this means learning a **mean** and **standard deviation** for each latent factor, rather than a simple value. Using the reparametrization trick, this is achieved by having two parallel layers representing the mean and std, followed by a sampling procedure to get `z`s.\n",
    "\n",
    "Finally, the KL-divergence of $q(z|x,w)$ from the prior $p(z)$ (a unit gaussian) needs to be added to the loss function.\n",
    "\n",
    "**Implement a VAE**.\n",
    "\n",
    "_Tip:_ Typically when implementing VAEs, a network is taught to estimate $\\mu$ and $log(\\sigma)$. This results in more stable estimates.\n",
    "\n",
    "_Hint:_ the KL divergence of a normally distributed $q(z|x,w)$ from a unit normal is given by:\n",
    "\n",
    "$$\n",
    "KL(\\mathcal{N}(\\mu, \\Sigma)||\\mathcal{N}(0,1)) = \\frac{1}{2} \\sum_k \\left( \\sigma_k + \\mu_k^2 - log(\\sigma_k) - 1 \\right)\n",
    "$$\n",
    "\n",
    "This term will need to be added to the model's **loss**, in addition to the reconstruction loss, to complete the ELBO. Hint: `model.add_loss?`.\n",
    "\n",
    "_Another hint:_ Sampling from a distribution (using the reparametrization trick) can be achieved using a custom function layer. This layer can take the $\\mu$ and $\\sigma$ as inputs, and sample from a distribution given by these parameters. Hint: `keras.layers.Lambda?` and `tf.random.normal?`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3A (MODERATE): VAE with deterministic encoder\n",
    "\n",
    "VAE encoders (the encoder network of a vae) don't produce encodings directly --- they produce, for each latent factor, the variance and mean of a normal distribution. It is by sampling from that distribution that latent factors are obtained. However, this means that every time the encoder is called, on the same data, it will produce a different sample of the latent space representation! While it is important to keep this behavior during training, keeping it in the _use_ phase (when we get latent space representations of data) can present a problem for reproducability.\n",
    "\n",
    "**How would you address this? What kind of a deterministic encoder model could be defined that makes use of the trained VAE?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 (EXTRA CHALLENGING): Contractive autoencoder (CAE)\n",
    "\n",
    "The CAE aims to give close latent representations to points which are close in input space. This is achieved by penalizing the frobenius norm of the jacobian of hidden neuron activations with respect to inputs, i.e. the gradient of the hidden layer with respect to the input. For an intuition behind this, we don't want small perturbations in the input to result in large perturbations in the latent space. The gradient of the hidden with respect to the input represents the size of latent space perturbations resulting from input perturbations. With a tuning parameter $\\lambda$, this results in a loss function $\\mathcal{L} = MSE + \\lambda\\|J_x(z)\\|_F$.\n",
    "\n",
    "**Implement a contractive autoencoder**.\n",
    "\n",
    "_Hint:_ Take a look at the TensorFlow subclassing API. Specifically, how to add losses: https://www.tensorflow.org/guide/keras/custom_layers_and_models#layers_recursively_collect_losses_created_during_the_forward_pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "\n",
    "## Deep autoencoders\n",
    "\n",
    "### Exercise 5 (EASY): Deep Vanilla Autoencoder (Stacked AEs)\n",
    "\n",
    "**Implement an n-layer vanilla autoencoder**.\n",
    "\n",
    "**Question: Is there a disadvantage to stacking AEs?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6 (CHALLENGING): Stacked Variational Autoencoders\n",
    "\n",
    "**Implement stacked VAEs**.\n",
    "\n",
    "**Variant 1: several \"regular\" layers with one \"variational\" layer in the middle**\n",
    "\n",
    "**Variant 2: variational at each layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "## Model Interpretation\n",
    "\n",
    "### Exercise 7 (EASY): Explain which genes are associated with each latent factor\n",
    "\n",
    "Using the weights from input genes to hidden neurons, we can explain which genes are behind each latent factor.\n",
    "\n",
    "**Question 1:** are l1-regularized autoencoders sparser than \"vanilla\" autoencoders? Can a relationship be established between the regularization parameter and the number of genes associated with each latent factor?\n",
    "\n",
    "**Question 2:** Are DAEs, VAEs, or CAEs better at creating sparser autoencoders?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code bere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8 (SLIGHTLY CHALLENGING): Explaining deep autoencoders\n",
    "\n",
    "**Question**: How would you establish the connection between input genes and latent factors in a deep autoencoder?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9 (SLIGHTLY CHALLENGING): pruning over-parameterized autoencoders\n",
    "\n",
    "We've seen the advantages of over-parameterizing autoencoders, e.g. by specifying a larger hidden dimension than we think is really latent in the data. The challenge remains to decide how to prune the \"noise\" latent factors in our model.\n",
    "\n",
    "1. Implement model pruning by _proportion of variance explained_, i.e. only keep those latent factors which explain a significant amount of variance in the input data.\n",
    "    1. (extra points) can this be done while training?\n",
    "2. Implement model pruning by gene set enrichment, i.e. associate each latent factor with genes, and only keep the ones which are associated with interesting gene sets\n",
    "3. Implement model pruning by clinical relevance, i.e. only keep latent factors which are predictive of patient survival\n",
    "4. Can you think of any other ways to subspace the large latent space we've inferred?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining models\n",
    "\n",
    "### Exercise 10 (CHALLENGING): Using a pre-trained autoencoder as a component in another neural network\n",
    "\n",
    "A major advantage of autoencoders being unsupervised learning techniques is, they do not require any labels, and can be used to learn patterns in data which is unlabeled. However, sometimes we want to train models to perform supervised learning tasks, e.g. classifying patients into a CMS subtype based on their molecular profiles.\n",
    "\n",
    "We have molecular profiles for more patients than we have subtype labels for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrna_patients = set(mrna.columns)\n",
    "subt_patients = set(subtypes[subtypes.cms_label.isin({f\"CMS{i}\" for i in range(1,5)})].index)\n",
    "print(f\"Out of {len(mrna_patients)} patients with mRNA seq, we have CMS labels for {len(mrna_patients & subt_patients)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be a shame to throw away the molecular profile of the 100 patients (representing 20% of the data at hand!), if we wanted to train a classifier to predict the CMS subtype from the mRNA profile.\n",
    "\n",
    "One way to make use of the data, is to **pre-train** an autoencoder on all of the data (both labeled and unlabeled). The resulting encoder network will have learned to create a meaningful representation of the input, and it will have made use of the unlabeled samples too in doing so. Then in the next step, we can use this pre-trained encoder network, put another dense/softmax layer at the end, and teach that thing to classify tumors into their CMS subtypes based on their mRNA profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11 (MODERATE): Multi-omics autoencoder\n",
    "\n",
    "We've also made available mutation and copy number data for the colorectal tumors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muts = pd.read_csv(os.path.join(DATADIR, \"crc-muts.csv.gz\"), index_col=0)\n",
    "cnvs = pd.read_csv(os.path.join(DATADIR, \"crc-cnvs.csv.gz\"), index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement a multi-omics integration autoencoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p36)",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
